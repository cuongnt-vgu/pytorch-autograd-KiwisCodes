{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention:\n",
    "For this lab, there are no local answers for the questions in learntools. However, there are some assertions for the first 4 questions. You should pass all of the assertions before submitting to the autograding.\n",
    "\n",
    "Some of the questions use mathematical symbols and equations to depict the scenario. These are just some basic \"formal\" definitions and you should understand them. In case you haven't understood them, read again and give yourself sometime to ponder. If there is any error or details in the questions that heavily confuse you, please post your concerns on Google Classroom. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Given a function $f(x) = x^2 + 3x + 8$, return the gradient of $x$ when $x=2.0$ using pytorch autograd. The gradient tensor should have type torch.float32 and dim=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_1() -> torch.Tensor:\n",
    "    # Define the input tensor x\n",
    "    # We need requires_grad=True to compute gradients with respect to x\n",
    "    # Set dtype to torch.float32 as required\n",
    "    # a tensor is like a multi-dimensional array commonly used for deep-learning\n",
    "    x = torch.tensor(2.0, dtype = torch.float32, requires_grad=True)\n",
    "    #the requires_grad tracks the computations related to x in the computational graph\n",
    "\n",
    "    # Define the function f(x) = x^2 + 3x + 8\n",
    "    f_x = x**2 + 3*x + 8\n",
    "    # x is a scalar, f_x is a scalar tensor, \n",
    "\n",
    "    # Compute the gradient using autograd\n",
    "    # backward() computes the gradient of the scalar tensor f_x with respect to x\n",
    "    f_x.backward()\n",
    "\n",
    "    # The gradient is stored in the .grad attribute of the tensor x\n",
    "    # print(x)\n",
    "    gradient_x = x.grad\n",
    "\n",
    "    # Print the gradient tensor\n",
    "    return gradient_x\n",
    "\n",
    "ex1_sol = exercise_1()\n",
    "assert torch.equal(ex1_sol, torch.tensor(7.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: A typical layer in a Neural Network would look like $f(x) = w_1 x_1 + w_2 x_2 + w_3 x_3 + ... + b$, where $w_j$ is the j-th weight of the function and $b$ is its bias. This function outputs a result for a multi-dimension input, where $x_j$ is the j-th feature of the input. Implement this function, return the gradients of each weight for a given input `x` and the output using `tuple` with the formar `(output, weight_grads_tensor)`. The weights should be randomized using `torch.randn()` and the bias must be set to $5.0$. The input used by the autograder will be the same size as the one given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "# this is usally dataset of many samples, the dimension is usually (batch_size, num_features)\n",
    "\n",
    "def exercise_2(x) -> tuple:\n",
    "    # all the output and gradients are tensors\n",
    "    torch.manual_seed(0) # must set seed=0 before random your weight\n",
    "    num_featues = x.shape[0]\n",
    "    # in this case x is only 1 datapoint and has all the features\n",
    "    # print(num_featues)\n",
    "    weights = torch.randn(num_featues, dtype = torch.float32, requires_grad=True )\n",
    "    bias = torch.tensor([5.0], dtype = torch.float32, requires_grad=True)\n",
    "    output = x.float() @ weights + bias \n",
    "    # @ is a matrix multiplication operator in pytorch\n",
    "    # basically it sees x of size (5,) and weights of size (5,) and\n",
    "    # @ treats x as (1,5) and weights as (5,1)\n",
    "    output.backward()\n",
    "    # this only calculate the gradients, not change any thing of the weights\n",
    "    weight_grads_tensor = weights.grad\n",
    "    return (output, weight_grads_tensor)\n",
    "    # you must return in this format\n",
    "    # return (output, weight_grads_tensor)\n",
    "\n",
    "ex2_sol = exercise_2(x)\n",
    "\n",
    "assert (ex2_sol[0] - torch.tensor(-3.7311) < 1e-4).item()\n",
    "assert torch.equal(ex2_sol[1], torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Suppose each feature $x_j$ needs a separate function $f_j(x_j)$, where $f_j(x_j) = w_j x_j + b_j$. In other words, each feature requires a function, which has a separate set of 1 weight and 1 bias. Therefore, the output $y$ of your function $G(x)=[f_1(x_1), f_2(x_2), ..., f_n(x_n)]$ should be $y = [(w_1 x_1 + b_1), (w_2 x_2 + b_2), ..., (w_n x_n + b_n)]$ (**note:** these are matrices written lazily with markdown). Implement the function $G(x)$, return the output and the gradient sets for each function for a given `x`. The weights should be randomized using `torch.randn()`, and the bias is the same as $j$ of $f_j()$ (for example: $b_1 = 1.0$, $b_2 = 2.0$). The input used by the autograder will be the same size as the one given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5], dtype = torch.float32)\n",
    "\n",
    "def exercise_3(x) -> tuple:\n",
    "     # all the output and gradients are tensors\n",
    "    \n",
    "    torch.manual_seed(0) # must set seed=0 before random your weight\n",
    "    num_features = x.shape[0]\n",
    "    weights = torch.randn(num_features, dtype=torch.float32, requires_grad=True)\n",
    "    bias = torch.tensor([x+1 for x in range(num_features)], dtype = torch.float32, requires_grad=True)\n",
    "    output = x * weights + bias\n",
    "    # We need a scalar to call .backward() on.\n",
    "    scalar_output_for_backward = output.sum()\n",
    "    scalar_output_for_backward.backward()\n",
    "\n",
    "    weights_grads_tensor = weights.grad\n",
    "    return (output, weights_grads_tensor)\n",
    "    # you must return in this format\n",
    "    # return (output, weight_grads_tensor)\n",
    "\n",
    "ex3_sol = exercise_3(x)\n",
    "assert torch.all((ex3_sol[0] - torch.tensor([ 2.5410,  1.4131, -3.5364,  6.2737, -0.4226 ], dtype=torch.float32)) < 5e-4).item()\n",
    "assert torch.equal(ex3_sol[1], torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32))\n",
    "# q3.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Using the function $f(x)$ similar to the one in question 2, implement the function with random weights and bias (must use `torch.manual_seed(0)` before each initialization). Calculate the Mean Squared Error between the `target` and your function prediction on the `data` as a loss metric, then return a tuple containing the gradients of your function weights and bias based on the calculated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])\n",
    "target = torch.tensor([2.0])\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def exercise_4(data, target) -> tuple:\n",
    "    # both gradients are tensors\n",
    "    torch.manual_seed(0)\n",
    "    num_features = data.shape[0]\n",
    "    weights = torch.randn(num_features, dtype=torch.float32, requires_grad=True)\n",
    "    # torch.manual_seed(0)\n",
    "    bias = torch.randn(1, dtype = torch.float32, requires_grad=True)\n",
    "    #they want me to use the randn here\n",
    "\n",
    "    prediction  = data.float() @ weights + bias\n",
    "    # print(prediction)\n",
    "    loss = F.mse_loss(prediction, target) #the dimension of prediction and target is (1,)\n",
    "    loss.backward()\n",
    "    weights_grads_tensor = weights.grad\n",
    "    bias_grads_tensor = bias.grad\n",
    "    return (weights_grads_tensor, bias_grads_tensor)\n",
    "    # you must return in this format\n",
    "    # return (weight_grads_tensor, bias_grads_tensor)\n",
    "\n",
    "\n",
    "\n",
    "ex4_sol = exercise_4(data, target)\n",
    "assert ex4_sol[0].dim() == 1 and ex4_sol[1].dim() == 1, print(\"All tensors must have dim == 1\")\n",
    "assert torch.all((ex4_sol[0] - torch.tensor([-85.1518, -127.7277, -170.3036, -212.8795, -255.4553, -298.0312, -340.6071, -383.1830, -425.7589, -468.3348], dtype=torch.float32)) < 5e-4)\n",
    "assert torch.all((ex4_sol[1] - torch.tensor([-42.5759], dtype=torch.float32)) < 5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: This question worth 40% the points of this lab. Your solution for this question will be graded manually by the TAs, hence you will be informed later for the total points for this lab after the deadline has been met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This question will not focus on getting the right results as your implementation is more important. Therefore there will be no local answer or autograding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a small neural network consisting 2 Linear layers, each followed by a Sigmoid activation function. The template code has been provided below including the return format (`x` should be replaced by the final `tensor` after the forward pass). Your code must use layers and functions provided by `pytorch`. Initialize all necessary components that can be used during the training phase including a Binary Cross Entropy Loss function and a Gradient Descent optimizer. Train your model for 2 epochs with a proper train loop, then make a prediction on a given `val_x` and return your prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the first Linear Layer can have any `in_features` and `out_features`, but the last Linear Layer must have `out_features=2`. Your model will consume the whole input for each epoch, so you don't need to separate the input into batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Context*: This part is not necessary for your implementation, but it could help you to understand the work that your network is simulating. The `train_x` consists grades for 5 subjects of 20 students, with the minimum grade is 1 and the maximum is 10. The problem is a classification task, when your model predicts if a student is good or bad based on their 5 grades. That is the reason why the final layer must have 2 output features, which correspond to the 2 classes \"good\" and \"bad\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "--- Data Overview ---\n",
      "Sample Training Input (train_x first 5 rows):\n",
      "tensor([[9., 5., 2., 2., 6.],\n",
      "        [2., 6., 6., 4., 7.],\n",
      "        [3., 6., 3., 5., 9.],\n",
      "        [1., 1., 7., 1., 5.],\n",
      "        [1., 2., 7., 9., 9.]])\n",
      "\n",
      "Sample Training Targets (train_y first 5 values):\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "(Note: train_y is 1.0 if sum of features > 30, else 0.0)\n",
      "\n",
      "Sample Validation Input (val_x first 5 rows):\n",
      "tensor([[5., 3., 7., 8., 4.],\n",
      "        [8., 6., 6., 3., 9.],\n",
      "        [6., 4., 8., 5., 5.],\n",
      "        [8., 5., 3., 6., 5.],\n",
      "        [6., 3., 9., 6., 5.]])\n",
      "------------------------------\n",
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "------------------------------\n",
      "\n",
      "--- Predictions on Validation Data ---\n",
      "Validation Input (val_x all rows):\n",
      "tensor([[5., 3., 7., 8., 4.],\n",
      "        [8., 6., 6., 3., 9.],\n",
      "        [6., 4., 8., 5., 5.],\n",
      "        [8., 5., 3., 6., 5.],\n",
      "        [6., 3., 9., 6., 5.],\n",
      "        [6., 6., 9., 8., 4.],\n",
      "        [6., 5., 6., 8., 4.],\n",
      "        [3., 9., 7., 6., 6.],\n",
      "        [4., 8., 6., 9., 6.],\n",
      "        [3., 3., 6., 6., 3.]])\n",
      "\n",
      "Predictions for val_x (ex5_sol):\n",
      "tensor([[0.5487, 0.4036],\n",
      "        [0.5762, 0.4444],\n",
      "        [0.5478, 0.4121],\n",
      "        [0.5602, 0.4460],\n",
      "        [0.5429, 0.3981],\n",
      "        [0.5333, 0.4201],\n",
      "        [0.5463, 0.4217],\n",
      "        [0.5165, 0.4734],\n",
      "        [0.5289, 0.4455],\n",
      "        [0.5391, 0.4120]])\n",
      "\n",
      "Shape of predictions: torch.Size([10, 2])\n",
      "\n",
      "Interpretation of predictions:\n",
      "These are raw probability scores (between 0 and 1) from the model's first output neuron.\n",
      "A value closer to 1 suggests the model predicts the condition (sum of features > 30) is true for that input.\n",
      "A value closer to 0 suggests the model predicts the condition is false.\n",
      "------------------------------\n",
      "\n",
      "For reference: True condition for val_x (sum > 30):\n",
      "tensor([0., 1., 0., 0., 0., 1., 0., 1., 1., 0.])\n",
      "(This is what the model is trying to predict for val_x based on its training)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# These are the data definitions from your template\n",
    "train_x = torch.randint(1, 10, (20, 5), dtype=torch.float32) # input\n",
    "train_y = (train_x.sum(1) > 30).float() # target, shape (20,)\n",
    "val_x = torch.randint(3, 10, (10, 5), dtype=torch.float32)\n",
    "\n",
    "def exercise_5(train_x: torch.Tensor, train_y: torch.Tensor, val_x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    class MyNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MyNetwork, self).__init__()\n",
    "            # Requirements: 2 Linear layers, each followed by Sigmoid.\n",
    "            # Input features for the first layer is 5 (from train_x.shape[1]).\n",
    "            # Let's choose hidden_features = 10 (arbitrary, as allowed).\n",
    "            # The final Linear Layer must have out_features=2.\n",
    "            input_dim = train_x.shape[1] # Should be 5\n",
    "            hidden_dim = 10\n",
    "            output_dim_internal = 2 # Second linear layer outputs 2 features\n",
    "\n",
    "            # Layer 1\n",
    "            self.linear_layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "            self.sigmoid_1 = nn.Sigmoid()\n",
    "\n",
    "            # Layer 2\n",
    "            self.linear_layer_2 = nn.Linear(hidden_dim, output_dim_internal)\n",
    "            self.sigmoid_2 = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            # layer 1\n",
    "            x = self.linear_layer_1(x)\n",
    "            x = self.sigmoid_1(x)\n",
    "\n",
    "            # layer 2\n",
    "            x = self.linear_layer_2(x)\n",
    "            x = self.sigmoid_2(x)  # After this, x has shape (batch_size, 2)\n",
    "\n",
    "            # As per template's \"return x[:,0]\", we select the first component of the 2-feature output.\n",
    "            # This makes the network's effective output a tensor of shape (batch_size,).\n",
    "            return x\n",
    "\n",
    "    # Loss Function: Binary Cross Entropy Loss (as per image description)\n",
    "    # This is suitable because the model's output (after x[:,0]) will be a probability (0-1),\n",
    "    # and train_y is also 0.0 or 1.0.\n",
    "    train_y = F.one_hot(train_y.long(), num_classes=2).float()\n",
    "    #use one_hot encoding to make sure the train_y is of size (20,2) to fit the loss_criterion\n",
    "    print(train_y)\n",
    "    loss_criterion = nn.BCELoss() # Renamed from 'loss' to avoid conflict in the loop\n",
    "\n",
    "    # Model Instantiation\n",
    "    model = MyNetwork()\n",
    "\n",
    "    # Optimizer: Gradient Descent (SGD - Stochastic Gradient Descent, as per image description)\n",
    "    learning_rate = 0.01 # A common learning rate\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop (2 epochs, as per image description and template)\n",
    "    model.train() # Set the model to training mode\n",
    "    for i in range(2):\n",
    "        # Forward pass: get predictions on training data\n",
    "        output = model(train_x) #this will return x of size (20,2)\n",
    "\n",
    "        # Calculate loss\n",
    "        current_epoch_loss = loss_criterion(output, train_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()             # Clear gradients from previous iteration\n",
    "        current_epoch_loss.backward()     # Compute gradients\n",
    "        optimizer.step()                  # Update model weights\n",
    "\n",
    "        # You can uncomment this to see the loss per epoch if needed\n",
    "        # print(f\"Epoch {i+1}/2, Loss: {current_epoch_loss.item():.4f}\")\n",
    "\n",
    "    # Prediction on val_x\n",
    "    model.eval() # Set the model to evaluation mode (important for some layers like Dropout, BatchNorm)\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        prediction_on_val_x = model(val_x) # `prediction_on_val_x` will have shape (10,)\n",
    "    #I only need to do this once, if I want to monitor this while training I can do it in the loop\n",
    "\n",
    "    # The function should return the prediction on val_x\n",
    "    return prediction_on_val_x\n",
    "\n",
    "\n",
    "ex5_sol = exercise_5(train_x=train_x, train_y=train_y, val_x=val_x)\n",
    "\n",
    "#used AI to print some debuging and testing codes\n",
    "# --- Test Run ---\n",
    "# print(\"--- Data Overview ---\")\n",
    "# print(\"Sample Training Input (train_x first 5 rows):\")\n",
    "# print(train_x[:5])\n",
    "# print(\"\\nSample Training Targets (train_y first 5 values):\")\n",
    "# print(train_y[:5])\n",
    "# print(f\"(Note: train_y is 1.0 if sum of features > 30, else 0.0)\")\n",
    "\n",
    "# print(\"\\nSample Validation Input (val_x first 5 rows):\")\n",
    "# print(val_x[:5])\n",
    "# print(\"-\" * 30)\n",
    "\n",
    "# print(\"-\" * 30)\n",
    "# print(\"\\n--- Predictions on Validation Data ---\")\n",
    "# print(\"Validation Input (val_x all rows):\")\n",
    "# print(val_x)\n",
    "# print(\"\\nPredictions for val_x (ex5_sol):\")\n",
    "# print(ex5_sol)\n",
    "# print(\"\\nShape of predictions:\", ex5_sol.shape)\n",
    "# print(\"\\nInterpretation of predictions:\")\n",
    "# print(\"These are raw probability scores (between 0 and 1) from the model's first output neuron.\")\n",
    "# print(\"A value closer to 1 suggests the model predicts the condition (sum of features > 30) is true for that input.\")\n",
    "# print(\"A value closer to 0 suggests the model predicts the condition is false.\")\n",
    "# print(\"-\" * 30)\n",
    "\n",
    "# # For a more direct comparison, let's calculate the \"true\" condition for val_x\n",
    "# # This is NOT part of the exercise_5 function, just for our test run observation\n",
    "# true_val_condition = (val_x.sum(1) > 30).float()\n",
    "# print(\"\\nFor reference: True condition for val_x (sum > 30):\")\n",
    "# print(true_val_condition)\n",
    "# print(\"(This is what the model is trying to predict for val_x based on its training)\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
